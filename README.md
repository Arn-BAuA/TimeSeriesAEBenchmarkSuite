# Time Series Autoencoder Benchmark Suite

## Contents

## About this Repo and Basic Overview

This repository functions as a basis to evaluate the performance of various autoencodertypes for time series reproduction.
This work is part of a project which is aimed at time series anomaly detection, so there is a special focus on anomalies in
some parts of the code.
There are three major buildingblocks to create an autoencoder, datasets, model and training algorithm:

* The Data - The Data must be representative for the underlieng probabilitydistribution of the data source
and, more important here, there must be sufficient information in the dataset to match the information needed
to train a certain type of autoencoder (e.g. roughly: an LSTM autoencoder need more data than a feed forward Autoencoder
of similar size)
* The Model -  The Model must be suited to catch the information in the distribution that generates the data, while
beeing robust enough to withstand noise.
* The Training algorithm - The combination of batching strategy and optimizer and other steps for the training, that are
important to find the best weights for the model.

### Basic Architecture of Benchmark.py

If these three buildingblocks are designed and tweaked the right way, good results will be archived.
The Codestructure of this repository is centered around these three blocks.

The Heart of the code is the Benchmark.py file. It is a wrapper, which coordinates the exchange of information between one
class and two functions that encapsulate these three blocks. The process encapsulated in the benchmark script is the following:

* The DatasetWrapper method encapsulates a dataset. As an argument, it just takes a wrapper specific dict of hyperparameters (Well come to that later) and a path for the output to the file system. It delivers three datasets: trainingset, validationset and testset. These are arrays of pytorch tensors of dimensionality d x n where d is the number of dimensions and n is the number of samples.
Characteristics that estimate the size and information in the sets are outputted to the file system at the specified path.
The files except a data folder that contains the raw versions of the wrapped dataset. THis folder is not included in the git repo (datasets are to large). TODO: Make an init.sh scipt that creates this folder.

* The Model is a class, that takes only a dict of hyperparameters in the constructor. It is a model that suits the autoencoder interface: Namely: It takes the input data and outputs the reconstruction.

* The Trainier is a method that takes a training and a validation set, a model, a dictionary of hyperparameters and a path for outputs to the file system. It than trains the model on training set and validates it with the testset. The trainer methode gathers a bunch of performace characteristics, example outputs and model weights at diffrent points of the training. They will be written to the filesystem in a specified location. The Trainermethod outputs the trained model.

The trained model will than be tested on the testset. Performance characteristics and weights of the trained model are saved to the file system.
The diffrent methods that implement the interfaces specified above are stored in the folders in the repo in separate python files. Often used hyperparameter combinations are stored along with them in a folder that must be named like the script containing the module itself.
The Benchmark method takes the used classes methods , paths and hyperparameterdicts as inputs

### Working with the Suite

The benchmark scipt has to be called form somewhere. We dont call it directly. The Experiment class specified in the Experiment.py file does this.
The experiment class can be seen as an abstract superclass for all experiments that will be conducted. Individual subclasses are stored in the experiments folder. The Experiments class has a set of methods to create a folderstructure in the Results folder for a experiment run. It also has code to specify which classes and methods should be used with what hyperparamters.
The paths used for further file system interaction in the benchmark script are specified here.

The scripts in the Evaluation dict are used to evaluate the data on the fs generated by the experiment. In evaluation is a subdirectory that is called Utility\_Plot which contains templates for often used plots and an directory called Utility\_Data which encapsulates some helpers for loading and saving data.

## The modules in detail:

### The Benchmark.py file

### The Experiment.py file

### SetWrappers/\*.py

### AEModels/\*.py

### Trainers/\*.py

### Experiments/\*.py

### Evaluation/\*.py

## Example Code
